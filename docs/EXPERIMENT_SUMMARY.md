# Korean Hate Speech Detection: Experimental Summary
# í•œêµ­ì–´ í˜ì˜¤ í‘œí˜„ íƒì§€: ì‹¤í—˜ ìš”ì•½

> ğŸŒ [English](#english-version) | [í•œêµ­ì–´](#korean-version)

---

<a name="english-version"></a>
## ğŸ“Š English Version

### Project Overview
Developed a high-performance hate speech detection system for Korean text using transformer-based models with advanced optimization techniques.

**Final Achievement: F1-Score 0.9383 (+2.82%p from baseline)**

---

### ğŸ”¬ Experimental Phases

#### Phase 1: Initial Model Selection (Baseline)
**Objective:** Identify the best performing pre-trained Korean language model

**Models Tested:**
- KcBERT: 0.9101 â­ (1st place)
- ELECTRA: 0.8950 (2nd place)
- KoBERT: 0.8850 (3rd place)
- RoBERTa: 0.8780 (4th place)

**Configuration:** lr=5e-5, bs=16, epochs=5

**Decision:** Selected KcBERT and ELECTRA for further optimization

---

#### Phase 2: Data Augmentation (AEDA)
**Objective:** Improve model robustness through data augmentation

**Method:** AEDA (An Easier Data Augmentation)
- Randomly inserts punctuation marks (.,;?!:) into sentences
- Increases training data diversity without changing semantic meaning

**Results:**
- KcBERT: 0.9101 â†’ 0.9267 **(+1.66%p)** ğŸ“ˆ
- ELECTRA: 0.8950 â†’ 0.9185 **(+2.35%p)** ğŸ“ˆ

**Impact:** Largest single improvement in the entire pipeline

---

#### Phase 3: Hyperparameter Tuning
**Objective:** Optimize training configuration for each model

**Parameters Tuned:**
- Learning rate: 2e-5 (KcBERT), 1e-5 (ELECTRA)
- Batch size: 32
- Epochs: 12 (KcBERT), 10 (ELECTRA)

**Results:**
- KcBERT: 0.9267 â†’ 0.9315 **(+0.48%p)** ğŸ“ˆ
- ELECTRA: 0.9185 â†’ 0.9185 (stable)

**Key Finding:** KcBERT showed better response to hyperparameter optimization

---

#### Phase 4: Task-Adaptive Pre-Training (TAPT)
**Objective:** Apply domain-specific pre-training to the best model

**Method:** Continued pre-training on unlabeled hate speech corpus using Masked Language Modeling

**Process:**
1. Collected domain-specific Korean hate speech texts
2. Pre-trained KcBERT with MLM objective
3. Fine-tuned on labeled classification task

**Results:**
- KcBERT: 0.9315 â†’ 0.9329 **(+0.14%p)** ğŸ“ˆ

**Insight:** Domain adaptation provides marginal but consistent improvement

---

#### Phase 5: Ensemble Learning
**Objective:** Combine complementary strengths of multiple models

**Strategy:** Soft voting ensemble
- Model 1: KcBERT (TAPT + Fine-tuned) - **Weight: 0.55**
- Model 2: ELECTRA (Fine-tuned) - **Weight: 0.45**

**Rationale:**
- KcBERT: Highest individual performance (0.9329)
- ELECTRA: Architectural diversity (discriminator-based)
- 55:45 ratio balances accuracy with diversity

**Final Results:**
- **Ensemble F1-Score: 0.9383** ğŸ¯
- Total improvement: **+2.82%p**
- Average confidence: **95.60%**

---

### ğŸ“ˆ Performance Summary

| Phase | Method | Best F1 | Improvement |
|-------|--------|---------|-------------|
| 1 | Baseline | 0.9101 | - |
| 2 | + AEDA | 0.9267 | +1.66%p |
| 3 | + Tuning | 0.9315 | +0.48%p |
| 4 | + TAPT | 0.9329 | +0.14%p |
| 5 | + Ensemble | **0.9383** | +0.54%p |

**Cumulative Improvement: 2.82 percentage points**

---

### ğŸ¯ Key Achievements

1. âœ… **Systematic Optimization:** Progressive improvement through 5 phases
2. âœ… **Data Efficiency:** AEDA provided largest single boost
3. âœ… **Model Selection:** Rigorous baseline comparison
4. âœ… **Advanced Techniques:** TAPT for domain adaptation
5. âœ… **Ensemble Strategy:** Optimized soft voting

---

### ğŸ›  Technical Highlights

- **Data Augmentation:** AEDA for Korean text
- **Domain Adaptation:** Task-Adaptive Pre-Training
- **Ensemble Method:** Weighted soft voting
- **Hyperparameter Optimization:** Model-specific tuning
- **Special Tokens:** 17 custom tokens for privacy masking

---

### ğŸ— Final Model Architecture

**Ensemble Configuration:**

**Primary Model (55%):** KcBERT-TAPT
- Base: beomi/kcbert-base
- TAPT: Domain-specific MLM pre-training
- Fine-tuning: lr=2e-5, bs=32, ep=12

**Secondary Model (45%):** ELECTRA
- Base: monologg/koelectra-small-discriminator
- Fine-tuning: lr=1e-5, bs=32, ep=10

**Prediction:** Weighted soft voting on probability distributions

---

### ğŸ’¡ Conclusion

This project demonstrates a comprehensive approach to building state-of-the-art hate speech detection through:

1. Rigorous model selection
2. Strategic data augmentation
3. Careful hyperparameter optimization
4. Domain-adaptive pre-training
5. Intelligent model ensembling

**Final F1-Score: 0.9383** - A significant achievement in Korean hate speech detection.

---

### ğŸ“Š Visualizations

See `results/complete_experiment_summary.png` for detailed performance charts.

---

### ğŸ“š References

See `docs/REFERENCES.md` for complete bibliography.

---
---

<a name="korean-version"></a>
## ğŸ“Š í•œêµ­ì–´ ë²„ì „

### í”„ë¡œì íŠ¸ ê°œìš”
íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ê³¼ ê³ ê¸‰ ìµœì í™” ê¸°ë²•ì„ í™œìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ê³ ì„±ëŠ¥ í˜ì˜¤ í‘œí˜„ íƒì§€ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

**ìµœì¢… ì„±ê³¼: F1-Score 0.9383 (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ +2.82%p)**

---

### ğŸ”¬ ì‹¤í—˜ ë‹¨ê³„

#### Phase 1: ì´ˆê¸° ëª¨ë¸ ì„ ì • (ë² ì´ìŠ¤ë¼ì¸)
**ëª©í‘œ:** ìµœê³  ì„±ëŠ¥ì˜ ì‚¬ì „í•™ìŠµëœ í•œêµ­ì–´ ì–¸ì–´ ëª¨ë¸ ì‹ë³„

**í…ŒìŠ¤íŠ¸í•œ ëª¨ë¸:**
- KcBERT: 0.9101 â­ (1ìœ„)
- ELECTRA: 0.8950 (2ìœ„)
- KoBERT: 0.8850 (3ìœ„)
- RoBERTa: 0.8780 (4ìœ„)

**ì„¤ì •:** lr=5e-5, bs=16, epochs=5

**ê²°ì •:** ì¶”ê°€ ìµœì í™”ë¥¼ ìœ„í•´ KcBERTì™€ ELECTRA ì„ ì •

---

#### Phase 2: ë°ì´í„° ì¦ê°• (AEDA)
**ëª©í‘œ:** ë°ì´í„° ì¦ê°•ì„ í†µí•œ ëª¨ë¸ ê²¬ê³ ì„± í–¥ìƒ

**ë°©ë²•:** AEDA (An Easier Data Augmentation)
- ë¬¸ì¥ì— êµ¬ë‘ì (.,;?!:)ì„ ë¬´ì‘ìœ„ë¡œ ì‚½ì…
- ì˜ë¯¸ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  í•™ìŠµ ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€

**ê²°ê³¼:**
- KcBERT: 0.9101 â†’ 0.9267 **(+1.66%p)** ğŸ“ˆ
- ELECTRA: 0.8950 â†’ 0.9185 **(+2.35%p)** ğŸ“ˆ

**ì˜í–¥:** ì „ì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ ê°€ì¥ í° ë‹¨ì¼ ê°œì„ 

---

#### Phase 3: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
**ëª©í‘œ:** ê° ëª¨ë¸ì˜ í•™ìŠµ ì„¤ì • ìµœì í™”

**íŠœë‹í•œ íŒŒë¼ë¯¸í„°:**
- Learning rate: 2e-5 (KcBERT), 1e-5 (ELECTRA)
- Batch size: 32
- Epochs: 12 (KcBERT), 10 (ELECTRA)

**ê²°ê³¼:**
- KcBERT: 0.9267 â†’ 0.9315 **(+0.48%p)** ğŸ“ˆ
- ELECTRA: 0.9185 â†’ 0.9185 (ì•ˆì •ì  ìœ ì§€)

**ì£¼ìš” ë°œê²¬:** KcBERTê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì— ë” ë‚˜ì€ ë°˜ì‘ì„ ë³´ì„

---

#### Phase 4: ì‘ì—… ì ì‘í˜• ì‚¬ì „í•™ìŠµ (TAPT)
**ëª©í‘œ:** ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì— ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „í•™ìŠµ ì ìš©

**ë°©ë²•:** Masked Language Modelingì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸” ì—†ëŠ” í˜ì˜¤ í‘œí˜„ ì½”í¼ìŠ¤ì—ì„œ ì§€ì† ì‚¬ì „í•™ìŠµ

**ê³¼ì •:**
1. ë„ë©”ì¸ íŠ¹í™” í•œêµ­ì–´ í˜ì˜¤ í‘œí˜„ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
2. MLM ëª©ì í•¨ìˆ˜ë¡œ KcBERT ì‚¬ì „í•™ìŠµ
3. ë ˆì´ë¸”ëœ ë¶„ë¥˜ ì‘ì—…ì— Fine-tuning

**ê²°ê³¼:**
- KcBERT: 0.9315 â†’ 0.9329 **(+0.14%p)** ï¿½ï¿½

**ì¸ì‚¬ì´íŠ¸:** ë„ë©”ì¸ ì ì‘ì´ ì‘ì§€ë§Œ ì¼ê´€ëœ ê°œì„  ì œê³µ

---

#### Phase 5: ì•™ìƒë¸” í•™ìŠµ
**ëª©í‘œ:** ì—¬ëŸ¬ ëª¨ë¸ì˜ ë³´ì™„ì  ê°•ì  ê²°í•©

**ì „ëµ:** Soft voting ì•™ìƒë¸”
- Model 1: KcBERT (TAPT + Fine-tuned) - **ê°€ì¤‘ì¹˜: 0.55**
- Model 2: ELECTRA (Fine-tuned) - **ê°€ì¤‘ì¹˜: 0.45**

**ê°€ì¤‘ì¹˜ ì„ ì • ê·¼ê±°:**
- KcBERT: ê°€ì¥ ë†’ì€ ê°œë³„ ì„±ëŠ¥ (0.9329)
- ELECTRA: ì•„í‚¤í…ì²˜ ë‹¤ì–‘ì„± (íŒë³„ê¸° ê¸°ë°˜)
- 55:45 ë¹„ìœ¨ë¡œ ì •í™•ë„ì™€ ë‹¤ì–‘ì„± ê· í˜•

**ìµœì¢… ê²°ê³¼:**
- **ì•™ìƒë¸” F1-Score: 0.9383** ğŸ¯
- ì´ ê°œì„ ë„: **+2.82%p**
- í‰ê·  ì‹ ë¢°ë„: **95.60%**

---

### ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½

| Phase | ë°©ë²• | ìµœê³  F1 | ê°œì„ ë„ |
|-------|------|---------|--------|
| 1 | ë² ì´ìŠ¤ë¼ì¸ | 0.9101 | - |
| 2 | + AEDA | 0.9267 | +1.66%p |
| 3 | + íŠœë‹ | 0.9315 | +0.48%p |
| 4 | + TAPT | 0.9329 | +0.14%p |
| 5 | + ì•™ìƒë¸” | **0.9383** | +0.54%p |

**ëˆ„ì  ê°œì„ : 2.82 percentage points**

---

### ğŸ¯ ì£¼ìš” ì„±ê³¼

1. âœ… **ì²´ê³„ì  ìµœì í™”:** 5ë‹¨ê³„ë¥¼ í†µí•œ ì ì§„ì  ê°œì„ 
2. âœ… **ë°ì´í„° íš¨ìœ¨ì„±:** AEDAê°€ ê°€ì¥ í° ë‹¨ì¼ í–¥ìƒ ì œê³µ
3. âœ… **ëª¨ë¸ ì„ ì •:** ì—„ê²©í•œ ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ
4. âœ… **ê³ ê¸‰ ê¸°ë²•:** ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ TAPT
5. âœ… **ì•™ìƒë¸” ì „ëµ:** ìµœì í™”ëœ soft voting

---

### ğŸ›  ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

- **ë°ì´í„° ì¦ê°•:** í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ AEDA
- **ë„ë©”ì¸ ì ì‘:** Task-Adaptive Pre-Training
- **ì•™ìƒë¸” ë°©ë²•:** ê°€ì¤‘ì¹˜ ê¸°ë°˜ soft voting
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”:** ëª¨ë¸ë³„ ë§ì¶¤ íŠœë‹
- **íŠ¹ìˆ˜ í† í°:** ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ìš© 17ê°œ ì»¤ìŠ¤í…€ í† í°

---

### ğŸ— ìµœì¢… ëª¨ë¸ ì•„í‚¤í…ì²˜

**ì•™ìƒë¸” êµ¬ì„±:**

**ì£¼ ëª¨ë¸ (55%):** KcBERT-TAPT
- Base: beomi/kcbert-base
- TAPT: ë„ë©”ì¸ íŠ¹í™” MLM ì‚¬ì „í•™ìŠµ
- Fine-tuning: lr=2e-5, bs=32, ep=12

**ë³´ì¡° ëª¨ë¸ (45%):** ELECTRA
- Base: monologg/koelectra-small-discriminator
- Fine-tuning: lr=1e-5, bs=32, ep=10

**ì˜ˆì¸¡:** í™•ë¥  ë¶„í¬ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ soft voting

---

### ğŸ’¡ ê²°ë¡ 

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì„ í†µí•´ ìµœì²¨ë‹¨ í˜ì˜¤ í‘œí˜„ íƒì§€ë¥¼ êµ¬ì¶•í•˜ëŠ” ì¢…í•©ì  ì ‘ê·¼ ë°©ì‹ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

1. ì—„ê²©í•œ ëª¨ë¸ ì„ ì •
2. ì „ëµì  ë°ì´í„° ì¦ê°•
3. ì„¸ì‹¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
4. ë„ë©”ì¸ ì ì‘í˜• ì‚¬ì „í•™ìŠµ
5. ì§€ëŠ¥ì  ëª¨ë¸ ì•™ìƒë¸”

**ìµœì¢… F1-Score: 0.9383** - í•œêµ­ì–´ í˜ì˜¤ í‘œí˜„ íƒì§€ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼.

---

### ğŸ“Š ì‹œê°í™”

ìƒì„¸í•œ ì„±ëŠ¥ ì°¨íŠ¸ëŠ” `results/complete_experiment_summary.png` ì°¸ì¡°

---

### ğŸ“š ì°¸ê³ ë¬¸í—Œ

ì „ì²´ ì°¸ê³ ë¬¸í—Œì€ `docs/REFERENCES.md` ì°¸ì¡°

