
# 프로젝트 회고 (Project Reflection)

### 프로젝트 타임라인

- Phase 1 (Baseline): 1일
- Phase 2 (AEDA): 1일  
- Phase 3 (Tuning): 2일
- Phase 4 (TAPT): 1일 (학습 6시간)
- Phase 5 (Ensemble): 반나절
- 디버깅 & 문서화: 2일

**총 소요: 약 1주일**

### 기술적 도전 : 디버깅 과정에서 배운 것들

**첫 번째 장애: "왜 CUDA 에러가 계속 나지?" **

처음엔 에러가 날 때마다 너무 스트레스를 받았다.
Special token을 추가했는데 자꾸 `CUDA error: device-side assert triggered` 에러가 발생했다. 
처음엔 GPU 메모리 문제인 줄 알고 batch size를 줄여봤지만 소용없었다.

Stack Overflow를 뒤지다가 vocab size 관련 이슈를 발견했고, 
`print(len(tokenizer))`, `print(model.config.vocab_size)`로 확인해보니 
30017 vs 30000으로 불일치하는 걸 알게 됐다.  Special Token 17개를 추가하니 토크나이저 
크기가 30017이고 모델 사전이라고 할 수 있는vocab_size는30000를 유지하며 예측시 30000번 이상
token_id에 접근하니 범위 초가 에러가 난 것이었다. 

`model.resize_token_embeddings(len(tokenizer))`로 해결했지만, 
이 과정에서 embedding layer가 어떻게 작동하는지 잘 이해하게 
됐다.  

**두 번째 장애: ELECTRA 저장이 안 된다**

KcBERT는 잘 저장되는데 ELECTRA만 계속 ValueError가 났다.
"tensor is not contiguous"라는 에러 메시지를 보고 메모리 구조 
문제임을 알았다.  열심히 시간들여 실험하고 저장이 안된거였다.  절망 절망~ 땀났다.

처음엔 `.contiguous()` 호출로 해결하려 했는데, 어떤 파라미터가 
문제인지 일일이 체크하는 게 비효율적이라 판단했다.
결국 `save_safetensors=False`로 실행해 봤다.

나중에 알고 보니 ELECTRA는 특정 텐서가 메모리에 비연속적으로 
저장되어 `use_fast=False` 설정이 필수였다. ELECTRA는 use_slow 였던 것이다.  safetensors 형식은 연속 메모리만 허용한다
방법이 2가지 있었는데 use_fast=False가 더 간단해서 사용했다.  
tokenizer와 모델의 미묘한 호환성 이슈를 직접 겪으며 배웠다.

**가장 답답했던 순간: 어느 게 최종 모델인지 모르겠다**

25개 넘는 실험을 돌리다 보니 checkpoint가 어디에 뭐가 있는지 혼란스러웠다.
`checkpoint-1200`, `checkpoint-2400`... 숫자만 봐서는 어떤 설정인지 알 수 없었다.

결국 내가 알아볼 수 있는 기록의 명명 규칙을 정했다:
- `{model_name}_{technique}_v{version}`
- 예: `kcbert_tapt_final_v2`

그리고 tokenizer를 모델과 함께 저장하는 습관을 들였다.
한 번 tokenizer 없어서 inference 못 한 경험이 있어서...
wandb에도 이런 형식으로 잘 로그가 되어서 보기에 편했다.

--------------------------------------------------------------------------------
## 이 프로젝트를 통해 진짜 배운 것들

### 실험 과정 : "구체적인 기록"이 전부다

처음엔 WandB 없이 baseline 모델을 실험했다. 
"lr=2e-5가 좋았던 것 같은데... 아니 1e-5였나?"
미리 계획 세우지 않고 대략 시작해본 결과를 구분하기 힘들어 혼란스러워했다.

WandB를 이용해 로그를 계속하고 나름의 네이밍 규칙을 세워
어떤 설정이 어떤 결과를 냈는지 한눈에 보이니,
"이 부분을 바꾸면 성능이 오르는구나" 패턴이 보였다.

일단 모든 경우의 수의 조합의 실험하기에는 리소스, 시간, 경비 모드
한계가 있어서 어느 정도 다른 팀이 한 내용 등을 참조하고 그를 바탕으로
내가 세운 계획을 수정하고 조합하면서 효율적인 실험을 하려고 시도했다.

**교훈:** 실험 관리 도구는 사치가 아니라 필수다.


### 작은 개선의 누적이 큰 차이를 만든다

- AEDA로 1.66%p 올랐을 때는 신났다.
하지만 TAPT에서 0.14%p밖에 안 올랐을 때는 실망했다.

"6시간이나 돌렸는데 겨우 이것밖에?"

하지만 앙상블까지 하고 나니 최종적으로 2.82%p가 올랐고,
Test에서는 3.28%p나 향상됐다.

각 단계가 작아 보여도 쌓이면 엄청난 차이가 된다는 걸 배웠다.

---------------------------------------------------------------------

### 에러 해결 : 에러는 불편한 현실, 그러나 가장 좋은 선생님

Vocab 크기 불일치 에러가 났을 때,
처음엔 "왜 이런 에러가 나는 거야?"라며 짜증났다.

하지만 이 에러 덕분에:
- Embedding layer 구조를 이해했다
- Special token이 모델에 어떻게 영향을 주는지 알았다
- Tokenizer와 모델의 싱크를 맞추는 게 왜 중요한지 체득했다

--------------------------------------------------------------------------------

### 실험 설계 논리
Dev F1-Score 0.9383이 나왔을 때 기뻤다.  그리고 최종 inference에서 0.9429
가 나왔다.  하지만 더 의미 있던 건 "왜 이 결과가 나왔는지" 단계별로 설명할 
수 있다는 것이다.

1. KcBERT를 주요 실험 대상으로 선정한 이유
초기 4개 baseline 모델 비교 실험에서 KcBERT가 0.9101로 1위였슴.  
댓글 데이터로 사전학습되어 혐오 표현 도메인과 가까웠기 때문으로 
분석 된다.  향후 앙상블까지 적용할 계획으로 BERT계열에서 적용 결과
1위 모델 그리고 다른 계열 중 결과 1위 모델을 혼합하여 다양성을 제공
해 실험하고자 했다.

2. AEDA가 효과적이었던 이유: 한국어 어순 자유로움  데이터 증강에서 의미 
손상도 없다.  (+1.66%p)로 가장 큰 성능 향상 보임

3. TAPT가 작지만 일관되게 개선한 이유: Task에 적합한 데이터만 선별해야
하는 특성상 연산량이 작다.

4. Fine Tuning은 사전 학습된 모델을 레이블 데이터로 바로 학습한 것으로 
적절한 learning rate 그리고 증강으로 데이터 사이즈가 커지면 Epoch를 더 
증가 시켜야 함을 알았다.  Batch Size는 처음 baseline 모델을 제외하고는
일관되게 32로 진행했고 learning rate, Epoch를 모델별로 다르게 적용해 보았다.
- 앙상블이 성능을 올린 이유: 모델 간 오류가 달라서.  그리고 높은 성능에
더 큰 가중치를 주되,  ELECTRA의 기여도를 살리기 위해 55:45 비율로 적용, 
결과적으로 추론 Test F1-Score(Micro)는 0.9429 기록

숫자 뒤의 원리를 이해하니, 
다음 프로젝트에서 뭘 시도해야 할지 보인다.

---

### 성과 : Test > Dev가 주는 안도감

Test에서 Dev보다 높게 나왔을 때(0.9429 vs 0.9383),
"과적합 안 됐구나" 하는 안도감이 밀려왔다.

Early Stopping(patience=3), 적절한 데이터 증강,
검증 데이터로 꾸준히 확인하며 학습한 결과였다.

단순히 높은 점수가 아니라 "일반화 잘 되는 모델"을 만들었다는 
게 이번 실험의 성과라고 본다.

---

### 혼자 하는 게 아니다

에러가 막막할 때 찾아본 것들:
- HuggingFace Forums
- GitHub Issues
- Stack Overflow
- 생성형 AI (클로드, Chat-GPT)

비슷한 문제를 겪은 사람들의 흔적이 큰 도움이 됐다.
나도 나중에 ERRORS.md로 문서화해서 다른 사람에게 도움이 되길 바란다.

---

### 아쉽지만 이번에 시도하지 못한 것들 ~~ 다음에 반드시 시도!!
1. Cross-validation: 단일 dev set 의존은 위험. 5-fold 하고 싶었지만 시간이 없었다.
2. KcBERT-large: 메모리만 충분하다면 성능 더 오를 듯
3. Focal Loss: 클래스 불균형 대응
4. Active Learning: 어려운 샘플 추가 학습

**더 깊이 파고들고 싶은 것:**
1. 왜 ELECTRA는 특정 패턴에서 KcBERT보다 강할까?
2. 완곡 표현 탐지는 어떻게 개선할까?

---

### 결론: 경험하고 시행착오하며 배운 점

이 프로젝트의 진짜 가치는 94.29%가 아니다.

- 체계적으로 실험하는 법을 배웠다.  좀더 계획과 논리를 
가지고 실험 효율을 높여보려는 시도였다.  
- 막막한 에러를 해결해 보는 끈기를 키웠다.  
- 에러가 없기를 바라는 것은....말도 안되는 헛된 기대라는 것을 배웠다.
- 여러 방법을 단계 단계 시행하고 작은 개선을 쌓아가는 인내를 배웠다. 
- 왜 이 방법이 효과적인지 알 수 있게 되었다.

**다음엔 이런 기법을을 다음 프로젝트에 다 적용해 보고 싶다.** 
