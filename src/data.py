import os
import pandas as pd
import torch


class hate_dataset(torch.utils.data.Dataset):
    """dataframeì„ torch dataset classë¡œ ë³€í™˜"""

    def __init__(self, hate_dataset, labels):
        self.dataset = hate_dataset
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.dataset.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


def load_data(dataset_dir):
    """csv fileì„ dataframeìœ¼ë¡œ load"""
    dataset = pd.read_csv(dataset_dir)
    print("dataframe ì˜ í˜•íƒœ")
    print("-" * 100)
    print(dataset.head())
    
    # ===== ë¼ë²¨ ê²€ì‚¬ ë° ì •ì œ (ì¶”ê°€!) =====
    if 'output' in dataset.columns:
        # NaNì´ ì•„ë‹Œ ê°’ì´ ìˆì„ ë•Œë§Œ ì²˜ë¦¬ (testëŠ” NaNì´ë¼ ê±´ë„ˆëœ€)
        if dataset['output'].notna().any():
            print(f"\nğŸ” ë¼ë²¨ ê²€ì‚¬: {dataset_dir}")
            print(f"  ì›ë³¸ í¬ê¸°: {len(dataset)}")
            print(f"  ë¼ë²¨ ì¢…ë¥˜: {sorted(dataset['output'].dropna().unique())}")
            print(f"  NaN ê°œìˆ˜: {dataset['output'].isna().sum()}")
            
            # 0ê³¼ 1ë§Œ ë‚¨ê¸°ê¸°
            before = len(dataset)
            dataset = dataset[dataset['output'].isin([0, 1])].copy()
            dataset = dataset.dropna(subset=['output']).copy()
            dataset['output'] = dataset['output'].astype(int)
            after = len(dataset)
            
            removed = before - after
            if removed > 0:
                print(f"  âš ï¸ ì œê±°ëœ ë°ì´í„°: {removed}ê°œ")
            print(f"  âœ… ì •ì œ í›„: {after}ê°œ\n")
    # ===== ì—¬ê¸°ê¹Œì§€ =====
    
    return dataset


def construct_tokenized_dataset(dataset, tokenizer, max_length):
    print("tokenizer ì— ë“¤ì–´ê°€ëŠ” ë°ì´í„° í˜•íƒœ")
    print(dataset["input"][:5])
    
    model_name = tokenizer.name_or_path
    print(f"í† í¬ë‚˜ì´ì € ëª¨ë¸ëª…: {model_name}")
    
    return_token_type_ids = True
    if "roberta" in model_name.lower():
        return_token_type_ids = False
        print("RoBERTa ê°ì§€: return_token_type_ids=False ì„¤ì •")

    tokenized_senetences = tokenizer(
        dataset["input"].tolist(),
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
        add_special_tokens=True,
        return_token_type_ids=return_token_type_ids,
    )
    
     # ë””ë²„ê¹…: í† í¬ë‚˜ì´ì§• ê²°ê³¼ í™•ì¸
    print("í† í¬ë‚˜ì´ì§• ê²°ê³¼ í‚¤ë“¤:", tokenized_senetences.keys())
    print("í† í¬ë‚˜ì´ì§• ê²°ê³¼ê°€ Noneì¸ê°€?", tokenized_senetences is None)
    
    # ===== í† í° ID ê²€ì¦ ì¶”ê°€ =====
    input_ids = tokenized_senetences['input_ids']
    vocab_size = tokenizer.vocab_size
    
    max_token_id = input_ids.max().item()
    min_token_id = input_ids.min().item()
    
    print(f"\nğŸ” í† í° ID ê²€ì¦:")
    print(f"  Vocab í¬ê¸°: {vocab_size} (0~{vocab_size-1})")
    print(f"  ì‹¤ì œ ë²”ìœ„: {min_token_id}~{max_token_id}")
    
    # ë²”ìœ„ ë²—ì–´ë‚œ í† í° ì°¾ê¸°
    invalid_mask = (input_ids >= vocab_size) | (input_ids < 0)
    invalid_count = invalid_mask.sum().item()
    
    if invalid_count > 0:
        print(f"  âš ï¸ ì˜ëª»ëœ í† í°: {invalid_count}ê°œ")
        # [UNK] í† í°ìœ¼ë¡œ êµì²´
        unk_token_id = tokenizer.unk_token_id
        print(f"  ğŸ”§ [UNK]({unk_token_id})ë¡œ êµì²´")
        input_ids[invalid_mask] = unk_token_id
        print(f"  âœ… êµì²´ ì™„ë£Œ! ìƒˆ ìµœëŒ€ê°’: {input_ids.max().item()}")
    else:
        print(f"  âœ… ëª¨ë“  í† í° ìœ íš¨\n")
    # ===== ì—¬ê¸°ê¹Œì§€ =====
    
    return tokenized_senetences


def prepare_dataset(dataset_dir, tokenizer, max_len):
    """í•™ìŠµ(train)ê³¼ í‰ê°€(test)ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„"""
    # load_data
    train_dataset = load_data(os.path.join(dataset_dir, "train.csv")) 
    valid_dataset = load_data(os.path.join(dataset_dir, "dev.csv"))
    test_dataset = load_data(os.path.join(dataset_dir, "test.csv"))
    print("--- data loading Done ---")
    
    # ===== ì „ì²´ ìš”ì•½ ì¶œë ¥ (ì¶”ê°€!) =====
    print("\n" + "="*50)
    print("ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ ìš”ì•½")
    print("="*50)
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(valid_dataset)}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ")
    print("="*50 + "\n")
    # ===== ì—¬ê¸°ê¹Œì§€ =====
    
    # split label
    train_label = train_dataset["output"].values
    valid_label = valid_dataset["output"].values
    test_label = test_dataset["output"].values

    # tokenizing dataset
    tokenized_train = construct_tokenized_dataset(train_dataset, tokenizer, max_len)
    tokenized_valid = construct_tokenized_dataset(valid_dataset, tokenizer, max_len)
    tokenized_test = construct_tokenized_dataset(test_dataset, tokenizer, max_len)
    print("--- data tokenizing Done ---")

    # make dataset for pytorch.
    hate_train_dataset = hate_dataset(tokenized_train, train_label)
    hate_valid_dataset = hate_dataset(tokenized_valid, valid_label)
    hate_test_dataset = hate_dataset(tokenized_test, test_label)
    print("--- pytorch dataset class Done ---")

    return hate_train_dataset, hate_valid_dataset, hate_test_dataset, test_dataset