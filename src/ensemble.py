import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import os

class TwoModelEnsemble:
    def __init__(self, model1_path, model2_path, weights=[0.55, 0.45]):
        """
        2ê°œ ëª¨ë¸ ì•™ìƒë¸”
        model1: KcBERT TAPT (ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        model2: ELECTRA Phase 2-4
        """
        self.device = torch.device('cpu')
    
        print("=== ëª¨ë¸ ë¡œë”© ===")
        print(f"Device: {self.device}\n")
    
        # Model 1: KcBERT TAPT
        print(f"1. KcBERT TAPT ë¡œë”©: {model1_path}")
        self.tokenizer1 = AutoTokenizer.from_pretrained(model1_path)
        self.model1 = AutoModelForSequenceClassification.from_pretrained(model1_path)
    
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸° ë§ì¶”ê¸°
        if len(self.tokenizer1) != self.model1.config.vocab_size:
            print(f"âš ï¸  í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€: í† í¬ë‚˜ì´ì € {len(self.tokenizer1)} vs ëª¨ë¸ {self.model1.config.vocab_size}")
            print("í† í¬ë‚˜ì´ì €ë¥¼ ëª¨ë¸ í¬ê¸°ì— ë§ì¶¤...")
            # í† í¬ë‚˜ì´ì €ë¥¼ ëª¨ë¸ì˜ vocab_sizeë¡œ í™•ì¥
            self.model1.resize_token_embeddings(self.model1.config.vocab_size)
    
        self.model1.to(self.device)
        self.model1.eval()
    
        # Model 2: ELECTRA
        print(f"2. ELECTRA ë¡œë”©: {model2_path}")
        self.tokenizer2 = AutoTokenizer.from_pretrained(model2_path)
        self.model2 = AutoModelForSequenceClassification.from_pretrained(model2_path)
        self.model2.to(self.device)
        self.model2.eval()
    
        # ê°€ì¤‘ì¹˜
        self.weights = weights
        print(f"\nê°€ì¤‘ì¹˜: KcBERT {weights[0]}, ELECTRA {weights[1]}")
        print("ë¡œë”© ì™„ë£Œ!\n")
        
    def predict(self, texts, batch_size=32):
        """Soft Voting ì˜ˆì¸¡"""
        all_probs = []
        
        # Model 1 ì˜ˆì¸¡
        print("1/2: KcBERT ì˜ˆì¸¡ ì¤‘...")
        probs1 = self._predict_single(self.model1, self.tokenizer1, texts, batch_size)
        all_probs.append(probs1)
        
        # Model 2 ì˜ˆì¸¡
        print("2/2: ELECTRA ì˜ˆì¸¡ ì¤‘...")
        probs2 = self._predict_single(self.model2, self.tokenizer2, texts, batch_size)
        all_probs.append(probs2)
        
        # Weighted Soft Voting
        print("\nì•™ìƒë¸” ì§‘ê³„ ì¤‘...")
        weighted_probs = (
            all_probs[0] * self.weights[0] + 
            all_probs[1] * self.weights[1]
        )
        
        predictions = np.argmax(weighted_probs, axis=1)
        confidence = np.max(weighted_probs, axis=1)
        
        return predictions, confidence, weighted_probs
    
    def _predict_single(self, model, tokenizer, texts, batch_size):
        """ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡"""
        encodings = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors='pt'
        )
        
        probs_list = []
        num_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in tqdm(range(0, len(texts), batch_size), total=num_batches):
            batch = {
                k: v[i:i+batch_size].to(self.device) 
                for k, v in encodings.items()
            }
            
            with torch.no_grad():
                outputs = model(**batch)
                probs = torch.softmax(outputs.logits, dim=-1)
                probs_list.append(probs.cpu().numpy())
        
        return np.vstack(probs_list)


def evaluate_ensemble(ensemble, test_df):
    """ì•™ìƒë¸” í‰ê°€"""
    from sklearn.metrics import accuracy_score, f1_score, classification_report
    
    texts = test_df['input'].tolist()
    labels = test_df['output'].values
    
    predictions, confidence, probs = ensemble.predict(texts)
    
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='micro')
    
    print("\n" + "="*50)
    print("ğŸ¯ ì•™ìƒë¸” ìµœì¢… ê²°ê³¼")
    print("="*50)
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"\ní‰ê·  ì‹ ë¢°ë„: {confidence.mean():.4f}")
    print(f"ìµœì†Œ ì‹ ë¢°ë„: {confidence.min():.4f}")
    print(f"ìµœëŒ€ ì‹ ë¢°ë„: {confidence.max():.4f}")
    
    print("\n" + "="*50)
    print("ğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸")
    print("="*50)
    print(classification_report(
        labels, predictions, 
        target_names=['Non-Hate', 'Hate'],
        digits=4
    ))
    
    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'predictions': predictions,
        'confidence': confidence
    }


if __name__ == "__main__":
    print("="*60)
    print("Phase 5: 2-Model Ensemble (KcBERT TAPT + ELECTRA)")
    print("="*60 + "\n")
    
    # ì•™ìƒë¸” ìƒì„±
    ensemble = TwoModelEnsemble(
        model1_path="./models/kcbert_tapt_final_fixed",
        model2_path="./models/electra_final_v2_fixed",
        weights=[0.55, 0.45]
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
    test_df = pd.read_csv('./NIKL_AU_2023_COMPETITION_v1.0/dev.csv')
    
    results = evaluate_ensemble(ensemble, test_df)
    
    # ê²°ê³¼ ì €ì¥
    test_df['ensemble_prediction'] = results['predictions']
    test_df['ensemble_confidence'] = results['confidence']
    test_df.to_csv('./ensemble_results.csv', index=False)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: ./ensemble_results.csv")
    print(f"âœ… ìµœì¢… F1-Score: {results['f1_score']:.4f}")
